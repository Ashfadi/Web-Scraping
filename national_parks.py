# -*- coding: utf-8 -*-
"""ass1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tPIAZK6AQfl_m5SOR0bYRXBEvs8yVUQY
"""

from bs4 import BeautifulSoup
import requests
import pandas as pd

# This function extract address, address, city, state, zipcode, phone# of each park
def Get_Park_Details(park):
  base_url_prefix = 'https://www.nps.gov'
  base_url_suffix = 'index.htm'

  url =  base_url_prefix + park['Page_Link'] + base_url_suffix
  data = requests.get(url) # extract details of park
  parks_page = BeautifulSoup(data.content, 'html.parser')

  #extract address and seperate into line1, line2, line 3
  address = parks_page.find("span",{"class":"street-address"})
  if address:
    addressData = address.getText()
    address_lines = addressData.splitlines()
    park['Address Line 1'] = len(address_lines)>1 and address_lines[1] or 'null'
    park['Line 2'] = len(address_lines)>2 and address_lines[2] or 'null'
    park['Line 3'] = len(address_lines)>3 and address_lines[3] or 'null'

  #extract city,state, zipcode and phone number
  city = parks_page.find("span",{"itemprop":"addressLocality"})
  if city:
    park['City'] = city.getText()

  state = parks_page.find("span",{"class":"region"})
  if state:
    park['State'] = state.getText()

  zipcode = parks_page.find("span",{"class":"postal-code"})
  if zipcode:
    park['Zip Code'] = zipcode.getText()

  phone_number = parks_page.find("span",{"class":"tel"})
  if phone_number:
    park['Phone Number'] = phone_number.getText()
  #extract social media links
  # social_links = parks_page.find("div",{"class":"ParkFooter-socialLinks"})
  
  return park
#********************* FUNCTION END *************************************


page = requests.get("https://www.nps.gov/index.htm")
soup = BeautifulSoup(page.content, 'html.parser')

# Retrieve the page with the drop-down list
text_loc = soup.find("ul",{"class":"dropdown-menu SearchBar-keywordSearch"})

#list of all states
all_states = text_loc.find_all("li")

links = [] #list of links of all states
for e_state in all_states:
    link = e_state.findChild('a')
    links.append(link['href'])

base_url = 'https://www.nps.gov'
parks_info = []
for index, link in enumerate(links):
  data = requests.get(base_url + link) # get parks list of each state
  parks_page = BeautifulSoup(data.content, 'html.parser')

  parks = parks_page.find("ul",{"id":"list_parks"}).findAll("li",{"class":"clearfix"})
  for e_park in parks:
    park = {}
    block = ['Fort Caroline','Chesapeake Bay']
    park['Name'] = e_park.find('h3').getText()
    park['Category'] = e_park.find('h2').getText()
    park['Description'] = e_park.find('p').getText()
    print(park['Name'])
    park['Page_Link'] = e_park.find('h3').findChild('a')['href']
    if park['Name'] not in block:
      park = Get_Park_Details(park)
      parks_info.append(park)
      print(len(parks_info))

# convert dictionary into dataframe
df = pd.DataFrame(parks_info, columns=['Name','Category','Description','Address Line 1','Line 2','Line 3','City','State','Zip Code','Phone Number','Page_Link'])
# save df into .csv file
df.to_csv('parks_info.csv')
